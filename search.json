[
  {
    "objectID": "pseudo_label.html",
    "href": "pseudo_label.html",
    "title": "RNAfold",
    "section": "",
    "text": "import pandas as pd\n\n\n\nsub = pd.read_parquet('SRRF.parquet').iloc[:-1,:]\n\ntest = pd.read_parquet('test_sequences.parquet')\n\n\ntest['length'] = test.sequence.str.len()\n\n\ntest.length.value_counts()\n\n207    1000000\n177     335823\n457       6000\n307       2000\nName: length, dtype: int64\n\n\n\ntest = test.query('future==1')\n\ntest['id'] = test.apply(lambda r: list(range(r.id_min,r.id_max+1)), axis=1)\n\nt = test[['sequence_id','id']]\n\nexplode = t.explode('id')\n\nexplode['id'] = explode.id.astype(int)\n\nexplode = explode.sort_values('id')\n\n\nmask = sub.id.isin(explode.id)\n\n\nsub2 = sub[mask]\n\n\nsub2.shape\n\n(101148736, 3)\n\n\n\nexplode.shape\n\n(210356000, 2)\n\n\n\nexplode.shape\n\n(210356000, 2)\n\n\n\nmerge = explode.merge(sub2,'left')\n\n\ndel sub\n\n\nagg = merge.groupby('sequence_id').agg({'reactivity_DMS_MaP': list,'reactivity_2A3_MaP':list})\n\n\ndms = agg.reactivity_DMS_MaP.apply(pd.Series)\n\n\nimport gc\ngc.collect()\n\n0\n\n\n\na3 = agg.reactivity_2A3_MaP.apply(pd.Series)\n\n\ndf = pd.read_parquet('train_data.parquet')\n\n\ndf.columns\n\nIndex(['sequence_id', 'sequence', 'experiment_type', 'dataset_name', 'reads',\n       'signal_to_noise', 'SN_filter', 'reactivity_0001', 'reactivity_0002',\n       'reactivity_0003',\n       ...\n       'reactivity_error_0197', 'reactivity_error_0198',\n       'reactivity_error_0199', 'reactivity_error_0200',\n       'reactivity_error_0201', 'reactivity_error_0202',\n       'reactivity_error_0203', 'reactivity_error_0204',\n       'reactivity_error_0205', 'reactivity_error_0206'],\n      dtype='object', length=419)\n\n\n\ncol_name = [f'reactivity_000{i}' for i in range(1,178)]\n\n\nlen(col_name)\n\n177\n\n\n\nlen(dms.columns)\n\n177\n\n\n\ndms.columns = col_name\na3.columns = col_name\n\n\ndms['experiment_type'] = 'DMS_MaP'\na3['experiment_type'] = '2A3_MaP'\n\n\ndms['signal_to_noise'] = 2\na3['signal_to_noise'] = 2\n\n\ndms.to_parquet('dms.parquet')\n\n\na3.to_parquet('a3.parquet')\n\n\ndata = pd.concat([dms.reset_index(),a3.reset_index()])\n\n\nseq = test[['sequence_id','sequence']]\n\n\ndata = data.merge(seq)\n\n\ndata.to_parquet('public.parquet')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RNAfold",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "RNAfold",
    "section": "Install",
    "text": "Install\npip install RNAfold"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "RNAfold",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "iafoss_starter.html",
    "href": "iafoss_starter.html",
    "title": "Download data",
    "section": "",
    "text": "Reference: Iafoss starter\ndownload kaggle.json from kaggle settings\n# !pip install kaggle\n# !mkdir ~/.kaggle\n# !cp kaggle.json ~/.kaggle/kaggle.json\n# !chmod 600 ~/.kaggle/kaggle.json\n# kaggle datasets download iafoss/stanford-ribonanza-rna-folding-converted\n# unzip stanford-ribonanza-rna-folding-converted.zip\n# !kaggle kernels output iafoss/rna-starter-0-186-lb"
  },
  {
    "objectID": "iafoss_starter.html#setup",
    "href": "iafoss_starter.html#setup",
    "title": "Download data",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport os, gc\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom fastai.vision.all import *\nfrom torch.cuda.amp import GradScaler, autocast\n\n\ndef flatten(o):\n    \"Concatenate all collections and items as a generator\"\n    for item in o:\n        if isinstance(o, dict): yield o[item]; continue\n        elif isinstance(item, str): yield item; continue\n        try: yield from flatten(item)\n        except TypeError: yield item\n\n\n@delegates(GradScaler)\nclass MixedPrecision(Callback):\n    \"Mixed precision training using Pytorch's `autocast` and `GradScaler`\"\n    order = 10\n    def __init__(self, **kwargs): self.kwargs = kwargs\n    def before_fit(self): \n        self.autocast,self.learn.scaler,self.scales = autocast(),GradScaler(**self.kwargs),L()\n    def before_batch(self): self.autocast.__enter__()\n    def after_pred(self):\n        if next(flatten(self.pred)).dtype==torch.float16: self.learn.pred = to_float(self.pred)\n    def after_loss(self): self.autocast.__exit__(None, None, None)\n    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n    def before_step(self):\n        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow. \"\n        self.skipped=True\n        self.scaler.step(self)\n        if self.skipped: raise CancelStepException()\n        self.scales.append(self.scaler.get_scale())\n    def after_step(self): self.learn.scaler.update()\n\n    @property \n    def param_groups(self): \n        \"Pretend to be an optimizer for `GradScaler`\"\n        return self.opt.param_groups\n    def step(self, *args, **kwargs): \n        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n        self.skipped=False\n    def after_fit(self): self.autocast,self.learn.scaler,self.scales = None,None,None\n\n\nimport fastai\nfastai.callback.fp16.MixedPrecision = MixedPrecision\n\n\nclass LenMatchBatchSampler(torch.utils.data.BatchSampler):\n    def __iter__(self):\n        buckets = [[]] * 100\n        yielded = 0\n\n        for idx in self.sampler:\n            s = self.sampler.data_source[idx]\n            if isinstance(s,tuple): L = s[0][\"mask\"].sum()\n            else: L = s[\"mask\"].sum()\n            L = max(1,torch.div(L, 16, rounding_mode='trunc')) \n            if len(buckets[L]) == 0:  buckets[L] = []\n            buckets[L].append(idx)\n            \n            if len(buckets[L]) == self.batch_size:\n                batch = list(buckets[L])\n                yield batch\n                yielded += 1\n                buckets[L] = []\n                \n        batch = []\n        leftover = [idx for bucket in buckets for idx in bucket]\n\n        for idx in leftover:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yielded += 1\n                yield batch\n                batch = []\n\n        if len(batch) &gt; 0 and not self.drop_last:\n            yielded += 1\n            yield batch\n            \ndef dict_to(x, device='cuda'):\n    return {k:x[k].to(device) for k in x}\n\ndef to_device(x, device='cuda'):\n    return tuple(dict_to(e,device) for e in x)\n\nclass DeviceDataLoader:\n    def __init__(self, dataloader, device='cuda'):\n        self.dataloader = dataloader\n        self.device = device\n    \n    def __len__(self):\n        return len(self.dataloader)\n    \n    def __iter__(self):\n        for batch in self.dataloader:\n            yield tuple(dict_to(x, self.device) for x in batch)\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim=16, M=10000):\n        super().__init__()\n        self.dim = dim\n        self.M = M\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(self.M) / half_dim\n        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n        emb = x[...,None] * emb[None,...]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass RNA_Model(nn.Module):\n    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n        super().__init__()\n        self.emb = nn.Embedding(4,dim)\n        self.pos_enc = SinusoidalPosEmb(dim)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n        self.proj_out = nn.Linear(dim,2)\n    \n    def forward(self, x0):\n        mask = x0['mask']\n        Lmax = mask.sum(-1).max()\n        mask = mask[:,:Lmax]\n        x = x0['seq'][:,:Lmax]\n        \n        pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n        pos = self.pos_enc(pos)\n        x = self.emb(x)\n        x = x + pos\n        \n        x = self.transformer(x, src_key_padding_mask=~mask)\n        x = self.proj_out(x)\n        \n        return x\n\n\ndef loss(pred,target):\n    p = pred[target['mask'][:,:pred.shape[1]]]\n    y = target['react'][target['mask']].clip(0,1)\n    loss = F.l1_loss(p, y, reduction='none')\n    loss = loss[~torch.isnan(loss)].mean()\n    \n    return loss\n\nclass MAE(Metric):\n    def __init__(self): \n        self.reset()\n        \n    def reset(self): \n        self.x,self.y = [],[]\n        \n    def accumulate(self, learn):\n        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]]\n        y = learn.y['react'][learn.y['mask']].clip(0,1)\n        self.x.append(x)\n        self.y.append(y)\n\n    @property\n    def value(self):\n        x,y = torch.cat(self.x,0),torch.cat(self.y,0)\n        loss = F.l1_loss(x, y, reduction='none')\n        loss = loss[~torch.isnan(loss)].mean()\n        return loss\n\n\nclass RNA_Dataset_Test(Dataset):\n    def __init__(self, df, mask_only=False, **kwargs):\n        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n        df['L'] = df.sequence.apply(len)\n        self.Lmax = df['L'].max()\n        self.df = df\n        self.mask_only = mask_only\n        \n    def __len__(self):\n        return len(self.df)  \n    \n    def __getitem__(self, idx):\n        id_min, id_max, seq = self.df.loc[idx, ['id_min','id_max','sequence']]\n        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n        L = len(seq)\n        mask[:L] = True\n        if self.mask_only: return {'mask':mask},{}\n        ids = np.arange(id_min,id_max+1)\n        \n        seq = [self.seq_map[s] for s in seq]\n        seq = np.array(seq)\n        seq = np.pad(seq,(0,self.Lmax-L))\n        ids = np.pad(ids,(0,self.Lmax-L), constant_values=-1)\n        \n        return {'seq':torch.from_numpy(seq), 'mask':mask}, \\\n               {'ids':ids}\n\n\nclass RNA_Dataset(Dataset):\n    def __init__(self, df, mode='train', seed=2023, fold=0, nfolds=4, \n                 mask_only=False, sn = 1,**kwargs):\n        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n        self.Lmax = 206\n        df['L'] = df.sequence.apply(len)\n        df_2A3 = df.loc[df.experiment_type=='2A3_MaP']\n        df_DMS = df.loc[df.experiment_type=='DMS_MaP']\n        \n        split = list(KFold(n_splits=nfolds, random_state=seed, \n                shuffle=True).split(df_2A3))[fold][0 if mode=='train' else 1]\n        df_2A3 = df_2A3.iloc[split].reset_index(drop=True)\n        df_DMS = df_DMS.iloc[split].reset_index(drop=True)\n        \n        if sn is not None:\n            m = (df_2A3['signal_to_noise'].values &gt;= sn) & (df_DMS['signal_to_noise'].values &gt;= sn)\n            df_2A3 = df_2A3.loc[m].reset_index(drop=True)\n            df_DMS = df_DMS.loc[m].reset_index(drop=True)\n        \n        self.seq = df_2A3['sequence'].values\n        self.L = df_2A3['L'].values\n        \n        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n                                 'reactivity_0' in c]].values\n        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n                                 'reactivity_0' in c]].values\n        # self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n        #                          'reactivity_error_0' in c]].values\n        # self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n        #                         'reactivity_error_0' in c]].values\n        self.sn_2A3 = df_2A3['signal_to_noise'].values\n        self.sn_DMS = df_DMS['signal_to_noise'].values\n        self.mask_only = mask_only\n        \n    def __len__(self):\n        return len(self.seq)  \n    \n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        if self.mask_only:\n            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n            mask[:len(seq)] = True\n            return {'mask':mask},{'mask':mask}\n        seq = [self.seq_map[s] for s in seq]\n        seq = np.array(seq)\n        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n        mask[:len(seq)] = True\n        seq = np.pad(seq,(0,self.Lmax-len(seq)))\n        \n        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n                                           self.react_DMS[idx]],-1))\n        # react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n        #                                        self.react_err_DMS[idx]],-1))\n        sn = torch.FloatTensor([self.sn_2A3[idx],self.sn_DMS[idx]])\n        \n        return {'seq':torch.from_numpy(seq), 'mask':mask}, \\\n               {'react':react,\n                # 'react_err':react_err,\n                'sn':sn, 'mask':mask}"
  },
  {
    "objectID": "iafoss_starter.html#run",
    "href": "iafoss_starter.html#run",
    "title": "Download data",
    "section": "Run",
    "text": "Run\n\nfold=0\nfname = 'example0'\nPATH = './'\nOUT = './'\nbs = 256\nnum_workers = 8\nSEED = 2023\nnfolds = 5\ndevice = 'cuda'\nsn=1\n\n\ndf = pd.read_parquet('train_data.parquet')\n\n\ndf2.columns = ['sequence_id','reactivity_0001',\n 'reactivity_0002',\n 'reactivity_0003',\n 'reactivity_0004',\n 'reactivity_0005',\n 'reactivity_0006',\n 'reactivity_0007',\n 'reactivity_0008',\n 'reactivity_0009',\n 'reactivity_0010',\n 'reactivity_0011',\n 'reactivity_0012',\n 'reactivity_0013',\n 'reactivity_0014',\n 'reactivity_0015',\n 'reactivity_0016',\n 'reactivity_0017',\n 'reactivity_0018',\n 'reactivity_0019',\n 'reactivity_0020',\n 'reactivity_0021',\n 'reactivity_0022',\n 'reactivity_0023',\n 'reactivity_0024',\n 'reactivity_0025',\n 'reactivity_0026',\n 'reactivity_0027',\n 'reactivity_0028',\n 'reactivity_0029',\n 'reactivity_0030',\n 'reactivity_0031',\n 'reactivity_0032',\n 'reactivity_0033',\n 'reactivity_0034',\n 'reactivity_0035',\n 'reactivity_0036',\n 'reactivity_0037',\n 'reactivity_0038',\n 'reactivity_0039',\n 'reactivity_0040',\n 'reactivity_0041',\n 'reactivity_0042',\n 'reactivity_0043',\n 'reactivity_0044',\n 'reactivity_0045',\n 'reactivity_0046',\n 'reactivity_0047',\n 'reactivity_0048',\n 'reactivity_0049',\n 'reactivity_0050',\n 'reactivity_0051',\n 'reactivity_0052',\n 'reactivity_0053',\n 'reactivity_0054',\n 'reactivity_0055',\n 'reactivity_0056',\n 'reactivity_0057',\n 'reactivity_0058',\n 'reactivity_0059',\n 'reactivity_0060',\n 'reactivity_0061',\n 'reactivity_0062',\n 'reactivity_0063',\n 'reactivity_0064',\n 'reactivity_0065',\n 'reactivity_0066',\n 'reactivity_0067',\n 'reactivity_0068',\n 'reactivity_0069',\n 'reactivity_0070',\n 'reactivity_0071',\n 'reactivity_0072',\n 'reactivity_0073',\n 'reactivity_0074',\n 'reactivity_0075',\n 'reactivity_0076',\n 'reactivity_0077',\n 'reactivity_0078',\n 'reactivity_0079',\n 'reactivity_0080',\n 'reactivity_0081',\n 'reactivity_0082',\n 'reactivity_0083',\n 'reactivity_0084',\n 'reactivity_0085',\n 'reactivity_0086',\n 'reactivity_0087',\n 'reactivity_0088',\n 'reactivity_0089',\n 'reactivity_0090',\n 'reactivity_0091',\n 'reactivity_0092',\n 'reactivity_0093',\n 'reactivity_0094',\n 'reactivity_0095',\n 'reactivity_0096',\n 'reactivity_0097',\n 'reactivity_0098',\n 'reactivity_0099',\n 'reactivity_0100',\n 'reactivity_0101',\n 'reactivity_0102',\n 'reactivity_0103',\n 'reactivity_0104',\n 'reactivity_0105',\n 'reactivity_0106',\n 'reactivity_0107',\n 'reactivity_0108',\n 'reactivity_0109',\n 'reactivity_0110',\n 'reactivity_0111',\n 'reactivity_0112',\n 'reactivity_0113',\n 'reactivity_0114',\n 'reactivity_0115',\n 'reactivity_0116',\n 'reactivity_0117',\n 'reactivity_0118',\n 'reactivity_0119',\n 'reactivity_0120',\n 'reactivity_0121',\n 'reactivity_0122',\n 'reactivity_0123',\n 'reactivity_0124',\n 'reactivity_0125',\n 'reactivity_0126',\n 'reactivity_0127',\n 'reactivity_0128',\n 'reactivity_0129',\n 'reactivity_0130',\n 'reactivity_0131',\n 'reactivity_0132',\n 'reactivity_0133',\n 'reactivity_0134',\n 'reactivity_0135',\n 'reactivity_0136',\n 'reactivity_0137',\n 'reactivity_0138',\n 'reactivity_0139',\n 'reactivity_0140',\n 'reactivity_0141',\n 'reactivity_0142',\n 'reactivity_0143',\n 'reactivity_0144',\n 'reactivity_0145',\n 'reactivity_0146',\n 'reactivity_0147',\n 'reactivity_0148',\n 'reactivity_0149',\n 'reactivity_0150',\n 'reactivity_0151',\n 'reactivity_0152',\n 'reactivity_0153',\n 'reactivity_0154',\n 'reactivity_0155',\n 'reactivity_0156',\n 'reactivity_0157',\n 'reactivity_0158',\n 'reactivity_0159',\n 'reactivity_0160',\n 'reactivity_0161',\n 'reactivity_0162',\n 'reactivity_0163',\n 'reactivity_0164',\n 'reactivity_0165',\n 'reactivity_0166',\n 'reactivity_0167',\n 'reactivity_0168',\n 'reactivity_0169',\n 'reactivity_0170',\n'reactivity_0171',\n 'reactivity_0172',\n 'reactivity_0173',\n 'reactivity_0174',\n 'reactivity_0175',\n 'reactivity_0176',\n 'reactivity_0177',\n 'experiment_type',\n 'signal_to_noise',\n 'sequence']\n\n\nseed_everything(SEED)\n\n\nds_train = RNA_Dataset(df, mode='train', fold=fold, nfolds=nfolds,sn=sn)\n\n\nds_train_len = RNA_Dataset(df, mode='train', fold=fold, \n            nfolds=nfolds, mask_only=True,sn=sn)\n\n\nlen(ds_train),len(ds_train_len)\n\n(413374, 413374)\n\n\n\nsampler_train = torch.utils.data.RandomSampler(ds_train_len) # randomly select samples\n\nlen_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=bs,\n            drop_last=True)\n\ndl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n            batch_sampler=len_sampler_train, num_workers=num_workers,\n            persistent_workers=True), device)\n\nds_val = RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds)\nds_val_len = RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds, \n           mask_only=True)\n\nsampler_val = torch.utils.data.SequentialSampler(ds_val_len)\nlen_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=bs, \n           drop_last=False)\n\ndl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n           batch_sampler=len_sampler_val, num_workers=num_workers), device)\ngc.collect()\n\n0\n\n\n\ndata = DataLoaders(dl_train,dl_val)\nmodel = RNA_Model()   \nmodel = model.to(device)\n# model.load_state_dict(torch.load('example0_0.pth',map_location=torch.device('cuda')))\nlearn = Learner(data, model, loss_func=loss,cbs=[GradientClip(3.0)],\n            metrics=[MAE()]).to_fp16()\n\n\n# learn.lr_find()\n\n\nlearn.fit_one_cycle(10, lr_max=1.4454397387453355e-05, pct_start=0.02)\ntorch.save(learn.model.state_dict(),os.path.join(OUT,f'model2.pth'))\ngc.collect()\n\n\n\n\n\n\n    \n      \n      60.00% [6/10 30:00&lt;20:00]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\ntime\n\n\n\n\n0\n0.218330\n0.234415\n0.234866\n05:01\n\n\n1\n0.201503\n0.232055\n0.232559\n04:58\n\n\n2\n0.193914\n0.231752\n0.232275\n04:59\n\n\n3\n0.190076\n0.231313\n0.231865\n05:00\n\n\n4\n0.187435\n0.230562\n0.231099\n04:59\n\n\n5\n0.186071\n0.229915\n0.230462\n05:00\n\n\n\n\n\n    \n      \n      0.87% [14/1614 00:02&lt;05:09 0.1851]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\ntorch.save(learn.model.state_dict(),os.path.join(OUT,f'model2.pth'))\ngc.collect()\n\n2938"
  },
  {
    "objectID": "iafoss_starter.html#inference",
    "href": "iafoss_starter.html#inference",
    "title": "Download data",
    "section": "Inference",
    "text": "Inference\n\ndf_test = pd.read_parquet('test_sequences.parquet')\n\n\nds = RNA_Dataset_Test(df_test)\ndl = DeviceDataLoader(torch.utils.data.DataLoader(ds, batch_size=bs, \n               shuffle=False, drop_last=False, num_workers=num_workers), device)\n\n\ndel df_test\ngc.collect()\n\n0\n\n\n\nmodel = RNA_Model()   \nmodel = model.to(device)\n# model.load_state_dict(torch.load('model_fintune.pth',map_location=torch.device('cpu')))\nmodel.load_state_dict(torch.load('model2.pth',map_location=torch.device('cpu')))\nmodel.eval()\n\nRNA_Model(\n  (emb): Embedding(4, 192)\n  (pos_enc): SinusoidalPosEmb()\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (6): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (7): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (8): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (9): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (10): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n      (11): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n        )\n        (linear1): Linear(in_features=192, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=768, out_features=192, bias=True)\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate=none)\n      )\n    )\n  )\n  (proj_out): Linear(in_features=192, out_features=2, bias=True)\n)\n\n\n\nimport gc\ngc.collect()\n\n9\n\n\n\nids,preds = [],[]\nfor x,y in tqdm(dl):\n    with torch.no_grad(),torch.cuda.amp.autocast():\n        # p = torch.stack([torch.nan_to_num(model(x)) for model in models]\n        #                 ,0).mean(0)\n        p = torch.nan_to_num(model(x))\n        \n    for idx, mask, pi in zip(y['ids'].cpu(), x['mask'].cpu(), p.cpu()):\n        ids.append(idx[mask])\n        preds.append(pi[mask[:pi.shape[0]]])\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5235/5250 [05:29&lt;00:02,  5.13it/s]\n\n\n\nids = torch.concat(ids)\npreds = torch.concat(preds)\n\n\ndf = pd.DataFrame({'id':ids.numpy(), 'reactivity_DMS_MaP':preds[:,1].numpy(), \n                   'reactivity_2A3_MaP':preds[:,0].numpy()})\n# df.to_csv('submission.csv', index=False, float_format='%.4f') # 6.5GB\n\n\ndf\n\n\ndf['reactivity_DMS_MaP']=df['reactivity_DMS_MaP'].astype(float)\ndf['reactivity_2A3_MaP']=df['reactivity_2A3_MaP'].astype(float)\n\n\ndf.to_parquet('submission.parquet') # 6.5GB"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  }
]