{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6fbc2ffe-896c-4f81-a28c-b5499ae3882e",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e72a36-f8f6-4132-986b-824449edd524",
   "metadata": {},
   "source": [
    "Reference: [Iafoss starter](https://www.kaggle.com/code/iafoss/rna-starter-0-186-lb/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5b5af-8c9d-48ac-a79f-d06a945f13e5",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c0dba-474f-4250-824f-2018de22751d",
   "metadata": {},
   "source": [
    "download kaggle.json from kaggle settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e9a9b4-0ee5-4663-bf6a-8fc27cd73729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/kaggle.json\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# kaggle datasets download iafoss/stanford-ribonanza-rna-folding-converted\n",
    "# unzip stanford-ribonanza-rna-folding-converted.zip\n",
    "# !kaggle kernels output iafoss/rna-starter-0-186-lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2596e7-9fe7-4d01-86f0-2c9790d47fcc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effd7f94-7b08-463c-94d6-9f2767c0ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, gc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastai.vision.all import *\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2866e830-c447-4449-a616-8f0655265831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(o):\n",
    "    \"Concatenate all collections and items as a generator\"\n",
    "    for item in o:\n",
    "        if isinstance(o, dict): yield o[item]; continue\n",
    "        elif isinstance(item, str): yield item; continue\n",
    "        try: yield from flatten(item)\n",
    "        except TypeError: yield item\n",
    "\n",
    "\n",
    "@delegates(GradScaler)\n",
    "class MixedPrecision(Callback):\n",
    "    \"Mixed precision training using Pytorch's `autocast` and `GradScaler`\"\n",
    "    order = 10\n",
    "    def __init__(self, **kwargs): self.kwargs = kwargs\n",
    "    def before_fit(self): \n",
    "        self.autocast,self.learn.scaler,self.scales = autocast(),GradScaler(**self.kwargs),L()\n",
    "    def before_batch(self): self.autocast.__enter__()\n",
    "    def after_pred(self):\n",
    "        if next(flatten(self.pred)).dtype==torch.float16: self.learn.pred = to_float(self.pred)\n",
    "    def after_loss(self): self.autocast.__exit__(None, None, None)\n",
    "    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n",
    "    def before_step(self):\n",
    "        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow. \"\n",
    "        self.skipped=True\n",
    "        self.scaler.step(self)\n",
    "        if self.skipped: raise CancelStepException()\n",
    "        self.scales.append(self.scaler.get_scale())\n",
    "    def after_step(self): self.learn.scaler.update()\n",
    "\n",
    "    @property \n",
    "    def param_groups(self): \n",
    "        \"Pretend to be an optimizer for `GradScaler`\"\n",
    "        return self.opt.param_groups\n",
    "    def step(self, *args, **kwargs): \n",
    "        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n",
    "        self.skipped=False\n",
    "    def after_fit(self): self.autocast,self.learn.scaler,self.scales = None,None,None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5534f4b6-ca9d-47c9-b066-084cd9564a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "fastai.callback.fp16.MixedPrecision = MixedPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca91f01-fd86-462d-b589-9e736056eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            s = self.sampler.data_source[idx]\n",
    "            if isinstance(s,tuple): L = s[0][\"mask\"].sum()\n",
    "            else: L = s[\"mask\"].sum()\n",
    "            L = max(1,torch.div(L, 16, rounding_mode='trunc')) \n",
    "            if len(buckets[L]) == 0:  buckets[L] = []\n",
    "            buckets[L].append(idx)\n",
    "            \n",
    "            if len(buckets[L]) == self.batch_size:\n",
    "                batch = list(buckets[L])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[L] = []\n",
    "                \n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "            \n",
    "def dict_to(x, device='cuda'):\n",
    "    return {k:x[k].to(device) for k in x}\n",
    "\n",
    "def to_device(x, device='cuda'):\n",
    "    return tuple(dict_to(e,device) for e in x)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, dataloader, device='cuda'):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(dict_to(x, self.device) for x in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce40f7a-cece-4ec3-b6b3-6fc8635d3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1975ad29-8625-49cd-8a3c-4c44cf3dd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RNA_Model(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4,dim)\n",
    "        self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        pos = self.pos_enc(pos)\n",
    "        x = self.emb(x)\n",
    "        x = x + pos\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed35fd64-2b1d-4fa0-809a-c4c67801050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']].clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class MAE(Metric):\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.x,self.y = [],[]\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]]\n",
    "        y = learn.y['react'][learn.y['mask']].clip(0,1)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        x,y = torch.cat(self.x,0),torch.cat(self.y,0)\n",
    "        loss = F.l1_loss(x, y, reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5755061-30cd-46de-ac9c-2ff9e6dfc565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA_Dataset_Test(Dataset):\n",
    "    def __init__(self, df, mask_only=False, **kwargs):\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        self.Lmax = df['L'].max()\n",
    "        self.df = df\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_min, id_max, seq = self.df.loc[idx, ['id_min','id_max','sequence']]\n",
    "        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "        L = len(seq)\n",
    "        mask[:L] = True\n",
    "        if self.mask_only: return {'mask':mask},{}\n",
    "        ids = np.arange(id_min,id_max+1)\n",
    "        \n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)\n",
    "        seq = np.pad(seq,(0,self.Lmax-L))\n",
    "        ids = np.pad(ids,(0,self.Lmax-L), constant_values=-1)\n",
    "        \n",
    "        return {'seq':torch.from_numpy(seq), 'mask':mask}, \\\n",
    "               {'ids':ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e224446-6c53-4bce-b3f6-a6e649db6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA_Dataset(Dataset):\n",
    "    def __init__(self, df, mode='train', seed=2023, fold=0, nfolds=4, \n",
    "                 mask_only=False, sn = 1,**kwargs):\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        self.Lmax = 206\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        df_2A3 = df.loc[df.experiment_type=='2A3_MaP']\n",
    "        df_DMS = df.loc[df.experiment_type=='DMS_MaP']\n",
    "        \n",
    "        split = list(KFold(n_splits=nfolds, random_state=seed, \n",
    "                shuffle=True).split(df_2A3))[fold][0 if mode=='train' else 1]\n",
    "        df_2A3 = df_2A3.iloc[split].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.iloc[split].reset_index(drop=True)\n",
    "        \n",
    "        if sn is not None:\n",
    "            m = (df_2A3['signal_to_noise'].values >= sn) & (df_DMS['signal_to_noise'].values >= sn)\n",
    "            df_2A3 = df_2A3.loc[m].reset_index(drop=True)\n",
    "            df_DMS = df_DMS.loc[m].reset_index(drop=True)\n",
    "        \n",
    "        self.seq = df_2A3['sequence'].values\n",
    "        self.L = df_2A3['L'].values\n",
    "        \n",
    "        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        # self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "        #                          'reactivity_error_0' in c]].values\n",
    "        # self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "        #                         'reactivity_error_0' in c]].values\n",
    "        self.sn_2A3 = df_2A3['signal_to_noise'].values\n",
    "        self.sn_DMS = df_DMS['signal_to_noise'].values\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq[idx]\n",
    "        if self.mask_only:\n",
    "            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "            mask[:len(seq)] = True\n",
    "            return {'mask':mask},{'mask':mask}\n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)\n",
    "        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "        mask[:len(seq)] = True\n",
    "        seq = np.pad(seq,(0,self.Lmax-len(seq)))\n",
    "        \n",
    "        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n",
    "                                           self.react_DMS[idx]],-1))\n",
    "        # react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n",
    "        #                                        self.react_err_DMS[idx]],-1))\n",
    "        sn = torch.FloatTensor([self.sn_2A3[idx],self.sn_DMS[idx]])\n",
    "        \n",
    "        return {'seq':torch.from_numpy(seq), 'mask':mask}, \\\n",
    "               {'react':react,\n",
    "                # 'react_err':react_err,\n",
    "                'sn':sn, 'mask':mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b1e29-2b72-4d3e-94bf-58d2953fe484",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281b7e9-35d5-4698-be13-e637d2f46ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=0\n",
    "fname = 'example0'\n",
    "PATH = './'\n",
    "OUT = './'\n",
    "bs = 256\n",
    "num_workers = 8\n",
    "SEED = 2023\n",
    "nfolds = 5\n",
    "device = 'cuda'\n",
    "sn=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc534666-7e67-41e9-9d3a-f7bd03a718d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1bd0f4d-420c-4547-8e2a-fde5c911826c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns = ['sequence_id','reactivity_0001',\n",
    " 'reactivity_0002',\n",
    " 'reactivity_0003',\n",
    " 'reactivity_0004',\n",
    " 'reactivity_0005',\n",
    " 'reactivity_0006',\n",
    " 'reactivity_0007',\n",
    " 'reactivity_0008',\n",
    " 'reactivity_0009',\n",
    " 'reactivity_0010',\n",
    " 'reactivity_0011',\n",
    " 'reactivity_0012',\n",
    " 'reactivity_0013',\n",
    " 'reactivity_0014',\n",
    " 'reactivity_0015',\n",
    " 'reactivity_0016',\n",
    " 'reactivity_0017',\n",
    " 'reactivity_0018',\n",
    " 'reactivity_0019',\n",
    " 'reactivity_0020',\n",
    " 'reactivity_0021',\n",
    " 'reactivity_0022',\n",
    " 'reactivity_0023',\n",
    " 'reactivity_0024',\n",
    " 'reactivity_0025',\n",
    " 'reactivity_0026',\n",
    " 'reactivity_0027',\n",
    " 'reactivity_0028',\n",
    " 'reactivity_0029',\n",
    " 'reactivity_0030',\n",
    " 'reactivity_0031',\n",
    " 'reactivity_0032',\n",
    " 'reactivity_0033',\n",
    " 'reactivity_0034',\n",
    " 'reactivity_0035',\n",
    " 'reactivity_0036',\n",
    " 'reactivity_0037',\n",
    " 'reactivity_0038',\n",
    " 'reactivity_0039',\n",
    " 'reactivity_0040',\n",
    " 'reactivity_0041',\n",
    " 'reactivity_0042',\n",
    " 'reactivity_0043',\n",
    " 'reactivity_0044',\n",
    " 'reactivity_0045',\n",
    " 'reactivity_0046',\n",
    " 'reactivity_0047',\n",
    " 'reactivity_0048',\n",
    " 'reactivity_0049',\n",
    " 'reactivity_0050',\n",
    " 'reactivity_0051',\n",
    " 'reactivity_0052',\n",
    " 'reactivity_0053',\n",
    " 'reactivity_0054',\n",
    " 'reactivity_0055',\n",
    " 'reactivity_0056',\n",
    " 'reactivity_0057',\n",
    " 'reactivity_0058',\n",
    " 'reactivity_0059',\n",
    " 'reactivity_0060',\n",
    " 'reactivity_0061',\n",
    " 'reactivity_0062',\n",
    " 'reactivity_0063',\n",
    " 'reactivity_0064',\n",
    " 'reactivity_0065',\n",
    " 'reactivity_0066',\n",
    " 'reactivity_0067',\n",
    " 'reactivity_0068',\n",
    " 'reactivity_0069',\n",
    " 'reactivity_0070',\n",
    " 'reactivity_0071',\n",
    " 'reactivity_0072',\n",
    " 'reactivity_0073',\n",
    " 'reactivity_0074',\n",
    " 'reactivity_0075',\n",
    " 'reactivity_0076',\n",
    " 'reactivity_0077',\n",
    " 'reactivity_0078',\n",
    " 'reactivity_0079',\n",
    " 'reactivity_0080',\n",
    " 'reactivity_0081',\n",
    " 'reactivity_0082',\n",
    " 'reactivity_0083',\n",
    " 'reactivity_0084',\n",
    " 'reactivity_0085',\n",
    " 'reactivity_0086',\n",
    " 'reactivity_0087',\n",
    " 'reactivity_0088',\n",
    " 'reactivity_0089',\n",
    " 'reactivity_0090',\n",
    " 'reactivity_0091',\n",
    " 'reactivity_0092',\n",
    " 'reactivity_0093',\n",
    " 'reactivity_0094',\n",
    " 'reactivity_0095',\n",
    " 'reactivity_0096',\n",
    " 'reactivity_0097',\n",
    " 'reactivity_0098',\n",
    " 'reactivity_0099',\n",
    " 'reactivity_0100',\n",
    " 'reactivity_0101',\n",
    " 'reactivity_0102',\n",
    " 'reactivity_0103',\n",
    " 'reactivity_0104',\n",
    " 'reactivity_0105',\n",
    " 'reactivity_0106',\n",
    " 'reactivity_0107',\n",
    " 'reactivity_0108',\n",
    " 'reactivity_0109',\n",
    " 'reactivity_0110',\n",
    " 'reactivity_0111',\n",
    " 'reactivity_0112',\n",
    " 'reactivity_0113',\n",
    " 'reactivity_0114',\n",
    " 'reactivity_0115',\n",
    " 'reactivity_0116',\n",
    " 'reactivity_0117',\n",
    " 'reactivity_0118',\n",
    " 'reactivity_0119',\n",
    " 'reactivity_0120',\n",
    " 'reactivity_0121',\n",
    " 'reactivity_0122',\n",
    " 'reactivity_0123',\n",
    " 'reactivity_0124',\n",
    " 'reactivity_0125',\n",
    " 'reactivity_0126',\n",
    " 'reactivity_0127',\n",
    " 'reactivity_0128',\n",
    " 'reactivity_0129',\n",
    " 'reactivity_0130',\n",
    " 'reactivity_0131',\n",
    " 'reactivity_0132',\n",
    " 'reactivity_0133',\n",
    " 'reactivity_0134',\n",
    " 'reactivity_0135',\n",
    " 'reactivity_0136',\n",
    " 'reactivity_0137',\n",
    " 'reactivity_0138',\n",
    " 'reactivity_0139',\n",
    " 'reactivity_0140',\n",
    " 'reactivity_0141',\n",
    " 'reactivity_0142',\n",
    " 'reactivity_0143',\n",
    " 'reactivity_0144',\n",
    " 'reactivity_0145',\n",
    " 'reactivity_0146',\n",
    " 'reactivity_0147',\n",
    " 'reactivity_0148',\n",
    " 'reactivity_0149',\n",
    " 'reactivity_0150',\n",
    " 'reactivity_0151',\n",
    " 'reactivity_0152',\n",
    " 'reactivity_0153',\n",
    " 'reactivity_0154',\n",
    " 'reactivity_0155',\n",
    " 'reactivity_0156',\n",
    " 'reactivity_0157',\n",
    " 'reactivity_0158',\n",
    " 'reactivity_0159',\n",
    " 'reactivity_0160',\n",
    " 'reactivity_0161',\n",
    " 'reactivity_0162',\n",
    " 'reactivity_0163',\n",
    " 'reactivity_0164',\n",
    " 'reactivity_0165',\n",
    " 'reactivity_0166',\n",
    " 'reactivity_0167',\n",
    " 'reactivity_0168',\n",
    " 'reactivity_0169',\n",
    " 'reactivity_0170',\n",
    "'reactivity_0171',\n",
    " 'reactivity_0172',\n",
    " 'reactivity_0173',\n",
    " 'reactivity_0174',\n",
    " 'reactivity_0175',\n",
    " 'reactivity_0176',\n",
    " 'reactivity_0177',\n",
    " 'experiment_type',\n",
    " 'signal_to_noise',\n",
    " 'sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf62f7e3-31dd-45b0-b9a2-3594e0e77431",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f3cd44-d1b1-4ff3-b129-d7bb072d345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = RNA_Dataset(df, mode='train', fold=fold, nfolds=nfolds,sn=sn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e3bc90-6072-40d8-a404-983bf856b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_len = RNA_Dataset(df, mode='train', fold=fold, \n",
    "            nfolds=nfolds, mask_only=True,sn=sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d558376-a6de-4b59-b414-6febb9b42d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413374, 413374)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train),len(ds_train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c63bdae-289c-4be4-ab8d-04ac367d340e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler_train = torch.utils.data.RandomSampler(ds_train_len) # randomly select samples\n",
    "\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=bs,\n",
    "            drop_last=True)\n",
    "\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=num_workers,\n",
    "            persistent_workers=True), device)\n",
    "\n",
    "ds_val = RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds)\n",
    "ds_val_len = RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds, \n",
    "           mask_only=True)\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=bs, \n",
    "           drop_last=False)\n",
    "\n",
    "dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "           batch_sampler=len_sampler_val, num_workers=num_workers), device)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b44ef1db-c36c-4b44-9500-721931cb7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoaders(dl_train,dl_val)\n",
    "model = RNA_Model()   \n",
    "model = model.to(device)\n",
    "# model.load_state_dict(torch.load('example0_0.pth',map_location=torch.device('cuda')))\n",
    "learn = Learner(data, model, loss_func=loss,cbs=[GradientClip(3.0)],\n",
    "            metrics=[MAE()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83464309-a6ea-46b2-b32a-e71058deef2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d201873c-f71a-48b4-8de9-5886f6966d2e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [6/10 30:00&lt;20:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218330</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.234866</td>\n",
       "      <td>05:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.201503</td>\n",
       "      <td>0.232055</td>\n",
       "      <td>0.232559</td>\n",
       "      <td>04:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193914</td>\n",
       "      <td>0.231752</td>\n",
       "      <td>0.232275</td>\n",
       "      <td>04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.190076</td>\n",
       "      <td>0.231313</td>\n",
       "      <td>0.231865</td>\n",
       "      <td>05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.187435</td>\n",
       "      <td>0.230562</td>\n",
       "      <td>0.231099</td>\n",
       "      <td>04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.186071</td>\n",
       "      <td>0.229915</td>\n",
       "      <td>0.230462</td>\n",
       "      <td>05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='14' class='' max='1614' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.87% [14/1614 00:02&lt;05:09 0.1851]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.4454397387453355e-05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpct_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(learn\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel2.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[38;5;241m/\u001b[39mdiv, lr_max, lr_max\u001b[38;5;241m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoms \u001b[38;5;28;01mif\u001b[39;00m moms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:256\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:245\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:231\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:227\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    225\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/learner.py:208\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(pred,target):\n\u001b[0;32m----> 2\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     y \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreact\u001b[39m\u001b[38;5;124m'\u001b[39m][target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(p, y, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, lr_max=1.4454397387453355e-05, pct_start=0.02)\n",
    "torch.save(learn.model.state_dict(),os.path.join(OUT,f'model2.pth'))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b474df2e-30c8-489f-abaf-6d2ebc8c20ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2938"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(learn.model.state_dict(),os.path.join(OUT,f'model2.pth'))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b695bf4e-6fc5-4617-9a1e-4318ad3a18eb",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f4b6f58-3383-4611-a8bd-b2e92de19fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('test_sequences.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f390d374-3067-4123-bfaf-bd51f258b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = RNA_Dataset_Test(df_test)\n",
    "dl = DeviceDataLoader(torch.utils.data.DataLoader(ds, batch_size=bs, \n",
    "               shuffle=False, drop_last=False, num_workers=num_workers), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec01a46d-d754-4055-a04a-9c99e27a46c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6897398-3500-44bc-9a0c-6234d15e3f94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNA_Model(\n",
       "  (emb): Embedding(4, 192)\n",
       "  (pos_enc): SinusoidalPosEmb()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (8): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (9): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (10): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "      (11): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate=none)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNA_Model()   \n",
    "model = model.to(device)\n",
    "# model.load_state_dict(torch.load('model_fintune.pth',map_location=torch.device('cpu')))\n",
    "model.load_state_dict(torch.load('model2.pth',map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e56cab3-4711-4e55-90e3-6726904b3d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2db2435a-8164-4bc0-9829-13403b66cd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8150514-4913-4ad7-84ba-dcf23e63cc96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5235/5250 [05:29<00:02,  5.13it/s]"
     ]
    }
   ],
   "source": [
    "ids,preds = [],[]\n",
    "for x,y in tqdm(dl):\n",
    "    with torch.no_grad(),torch.cuda.amp.autocast():\n",
    "        # p = torch.stack([torch.nan_to_num(model(x)) for model in models]\n",
    "        #                 ,0).mean(0)\n",
    "        p = torch.nan_to_num(model(x))\n",
    "        \n",
    "    for idx, mask, pi in zip(y['ids'].cpu(), x['mask'].cpu(), p.cpu()):\n",
    "        ids.append(idx[mask])\n",
    "        preds.append(pi[mask[:pi.shape[0]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23416623-9529-4cf9-bae0-1b15fbf0c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.concat(ids)\n",
    "preds = torch.concat(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a7e2d-ff4d-443f-88b6-22c8ecff65c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':ids.numpy(), 'reactivity_DMS_MaP':preds[:,1].numpy(), \n",
    "                   'reactivity_2A3_MaP':preds[:,0].numpy()})\n",
    "# df.to_csv('submission.csv', index=False, float_format='%.4f') # 6.5GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800343c-1cac-4eab-ba47-4de2a4bc8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84213387-6bd0-4a16-954b-998173f1d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reactivity_DMS_MaP']=df['reactivity_DMS_MaP'].astype(float)\n",
    "df['reactivity_2A3_MaP']=df['reactivity_2A3_MaP'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f2abc-70d7-4421-903a-91bde5336ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('submission.parquet') # 6.5GB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
